# Basic AI Math Literacy Tutorial

## Introduction
Understanding AI requires familiarity with several mathematical concepts. This tutorial covers the essential math literacy needed to comprehend how AI systems work, from neural networks to machine learning algorithms.

---

## 1. Linear Algebra Fundamentals

### 1.1 Vectors
A vector is an ordered list of numbers representing a point in space.

**Example:**
- Vector in 2D: `v = [3, 4]` represents a point 3 units along the x-axis and 4 units along the y-axis
- Vector in 3D: `v = [1, 2, 3]`
- High-dimensional vector: `v = [0.1, 0.5, 0.3, 0.8]` (common in AI)

**Use in AI:** Vectors represent data points. A movie recommendation system might use vectors like `[genre_score, rating_score, year_score]` to represent each movie.

### 1.2 Matrices
A matrix is a 2D array of numbers (rows and columns).

**Example:**
```
    [1  2  3]
A = [4  5  6]
    [7  8  9]
```

**Use in AI:** Neural networks use matrices to store weights and transform data. An image with 784 pixels can be represented as a 28×28 matrix.

### 1.3 Matrix Multiplication
When multiplying matrices, multiply rows by columns.

**Example:**
```
[1 2]   [5 6]     [1×5+2×7  1×6+2×8]   [19 22]
[3 4] × [7 8]  =  [3×5+4×7  3×6+4×8] = [43 50]
```

**Use in AI:** Forward pass in neural networks uses matrix multiplication to transform input data through layers.

---

## 2. Calculus Concepts

### 2.1 Derivatives (Slopes)
A derivative measures how a function changes. It's the slope of a curve at a specific point.

**Example:**
- For `f(x) = x²`, the derivative is `f'(x) = 2x`
- At `x = 3`, the slope is `2(3) = 6`, meaning the function is increasing steeply

**Use in AI:** Derivatives tell us which direction to adjust weights in a neural network to reduce errors.

### 2.2 Gradient Descent
An optimization technique that uses derivatives to find the minimum of a function by taking small steps downhill.

**Example:**
```
Start: Weight = 5, Error = 10
Step 1: Weight = 4.8, Error = 8.5 (moving in better direction)
Step 2: Weight = 4.6, Error = 7.1
...eventually...
Final: Weight = 2.3, Error = 0.1 (minimum found)
```

**Use in AI:** This is how neural networks learn—by adjusting weights to minimize prediction errors.

### 2.3 Partial Derivatives
When a function has multiple variables, a partial derivative shows how one variable affects the output while keeping others constant.

**Example:**
- Function: `f(x, y) = x² + 3xy + y²`
- Partial derivative with respect to x: `∂f/∂x = 2x + 3y`
- Partial derivative with respect to y: `∂f/∂y = 3x + 2y`

**Use in AI:** Neural networks have thousands of weights. Partial derivatives tell us how to adjust each weight independently.

---

## 3. Probability & Statistics

### 3.1 Probability
The likelihood of an event occurring, expressed as a number between 0 and 1.

**Examples:**
- Coin flip: P(heads) = 0.5
- Rolling a die and getting 6: P(6) = 1/6 ≈ 0.167
- AI confidence: P(image is a cat) = 0.95 (95% confident)

### 3.2 Probability Distributions
A function describing the likelihood of different outcomes.

**Normal (Gaussian) Distribution:**
A bell-shaped curve where values cluster around a mean.

Example: Heights in a population
```
        |      /\
        |    /    \
 Count  |  /        \
        |/____________\
        158 165 172 179 (height in cm)
        Average: 172cm
```

**Use in AI:** Assumes data follows normal distribution. Used in Bayesian methods and initializing neural networks.

### 3.3 Expected Value
The average outcome you'd expect from many repetitions of an experiment.

**Example:**
- Rolling a fair die: E(value) = (1+2+3+4+5+6)/6 = 3.5
- Betting $10 on a coin flip where you get $15 if heads: E(return) = 0.5×15 + 0.5×0 = $7.50

**Use in AI:** Fundamental to reinforcement learning and decision-making algorithms.

---

## 4. Logarithms & Exponentials

### 4.1 Exponentials
A function where a base is raised to a power.

**Examples:**
- `2³ = 2 × 2 × 2 = 8`
- `e^x` is the natural exponential (e ≈ 2.718)
- AI context: Softmax uses `e^x` to convert scores to probabilities

### 4.2 Logarithms
The inverse of exponentials. If `b^x = a`, then `log_b(a) = x`.

**Examples:**
- `log₂(8) = 3` (because 2³ = 8)
- `log₁₀(100) = 2` (because 10² = 100)
- Natural log: `ln(e) = 1`

**Use in AI:** 
- Cross-entropy loss uses logarithms to measure prediction errors
- Information gain in decision trees uses logarithms

---

## 5. Key AI-Specific Concepts

### 5.1 Loss Functions
Measures how wrong a model is. Lower is better.

**Example: Mean Squared Error (MSE)**
```
Predictions:  [2.5, 3.1, 1.9]
Actual:       [2.0, 3.5, 2.0]
Errors:       [0.5, -0.4, -0.1]
Squared:      [0.25, 0.16, 0.01]
MSE = (0.25 + 0.16 + 0.01) / 3 = 0.14
```

### 5.2 Activation Functions
Non-linear functions that add complexity to neural networks, allowing them to learn non-linear patterns.

**ReLU (Rectified Linear Unit):**
```
f(x) = max(0, x)

Example:
f(-2) = 0
f(0) = 0
f(3.5) = 3.5
```

**Sigmoid:**
```
f(x) = 1 / (1 + e^(-x))

Example:
f(-5) ≈ 0.007   (very close to 0)
f(0) = 0.5      (middle)
f(5) ≈ 0.993    (very close to 1)

Use: Converts values to probability-like outputs (0 to 1)
```

### 5.3 Backpropagation
The algorithm that uses the chain rule (from calculus) to compute how much each weight contributed to the error.

**Simplified Example:**
```
Input → Weight₁ → Activation → Weight₂ → Output → Loss
                                             ↑
                       How much did each weight affect this error?
                       Backpropagation calculates this.
```

### 5.4 Softmax
Converts model outputs into probabilities that sum to 1.

**Example:**
```
Raw model output:  [2.0, 1.0, 0.1]
After softmax:     [0.659, 0.242, 0.099]
                   (sums to 1.0, interpretable as probabilities)
```

---

## 6. Practical Examples

### Example 1: Simple Linear Regression
Predicting house prices from square footage.

```
Data:
Square Feet    Price
1000          $150,000
1500          $225,000
2000          $300,000

Linear equation: Price = m × SquareFeet + b
Goal: Find best m (slope) and b (intercept)

Solution: m = 150, b = 0
Predictions:
1000 sq ft → $150,000 ✓
1500 sq ft → $225,000 ✓
2000 sq ft → $300,000 ✓
```

### Example 2: Classification with Probabilities
Classifying an email as spam or not spam.

```
Model outputs:
Raw scores:     [0.8, 0.2]   (might be logits)

After softmax:
Not spam:       0.73 (73%)
Spam:           0.27 (27%)

Decision: Classify as "Not spam" (higher probability)
```

### Example 3: Computing Prediction Error
A model predicting student test scores.

```
Actual scores:     [85, 90, 78]
Predicted scores:  [82, 92, 79]
Errors:            [3, -2, -1]

Absolute error:    3 + 2 + 1 = 6
Mean absolute error (MAE): 6 / 3 = 2 points

This tells us: on average, predictions are off by 2 points
```

---

## 7. Common Mathematical Notation

| Symbol | Meaning | Example |
|--------|---------|---------|
| Σ | Sum | Σ(1,2,3) = 6 |
| ∏ | Product | ∏(2,3,4) = 24 |
| ∂ | Partial derivative | ∂f/∂x |
| ∇ | Gradient | ∇f = [∂f/∂x, ∂f/∂y] |
| × | Matrix multiplication | A × B |
| ⊙ | Element-wise multiplication | [1,2] ⊙ [3,4] = [3,8] |
| || || | Norm (magnitude) | ||v|| = √(v₁² + v₂²) |

---

## 8. Practice Exercises

### Exercise 1: Vectors
Create two vectors representing student test scores in Math and English:
- Student A: Math = 85, English = 78
- Student B: Math = 92, English = 88

Calculate the distance between these students' performance profiles.

**Solution:**
```
v_A = [85, 78]
v_B = [92, 88]
Difference = [92-85, 88-78] = [7, 10]
Distance = √(7² + 10²) = √(49 + 100) = √149 ≈ 12.2
```

### Exercise 2: Probability
If a diagnostic test has:
- 95% accuracy for people who have a disease
- 90% accuracy for people who don't

What's the probability someone actually has the disease if they test positive?

**Hint:** Use Bayes' theorem: P(disease|positive) = P(positive|disease) × P(disease) / P(positive)

### Exercise 3: Gradient Descent
A function f(x) = (x-3)² has a minimum at x=3. Starting at x=10:
- Step size = 0.5
- Derivative at x=10 is 2(10-3) = 14

What's the next value of x?
**Solution:** x_new = 10 - 0.5 × 14 = 3

---

## 9. Key Takeaways

1. **Vectors & Matrices** represent data and transformations
2. **Calculus** (derivatives) helps optimize AI models
3. **Probability** quantifies uncertainty in predictions
4. **Logarithms** measure information and error
5. **Activation functions** enable complex learning
6. **Backpropagation** efficiently updates millions of parameters

---

## 10. Further Learning Resources

- **Linear Algebra:** 3Blue1Brown's "Essence of Linear Algebra" (YouTube)
- **Calculus:** 3Blue1Brown's "Essence of Calculus" (YouTube)
- **Statistics:** Khan Academy's Statistics & Probability
- **Deep Learning Math:** "Deep Learning" by Goodfellow, Bengio, and Courville
- **Interactive:** Desmos.com for visualizing functions

---

## Conclusion

AI math isn't about memorizing formulas—it's about understanding how these mathematical tools work together to enable machines to learn from data. Start with these fundamentals and gradually explore more advanced topics as you build AI projects.

The key is to connect the math to real-world problems and visualize concepts whenever possible.
