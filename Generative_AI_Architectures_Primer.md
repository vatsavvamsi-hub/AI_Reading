# Generative AI Architectures: A Comprehensive Primer

---

## Table of Contents

1. [Transformer Architecture](#1-transformer-architecture)
2. [Diffusion Models](#2-diffusion-models)
3. [Generative Adversarial Networks (GANs)](#3-generative-adversarial-networks-gans)
4. [Variational Autoencoders (VAEs)](#4-variational-autoencoders-vaes)
5. [Multimodal Vision-Language Models](#5-multimodal-vision-language-models)
6. [Hybrid and Emerging Architectures](#6-hybrid-and-emerging-architectures)
7. [Architecture Comparison](#7-architecture-comparison)

---

## 1. Transformer Architecture

### Overview

The Transformer was proposed in 2017 by Google researchers in the landmark paper **"Attention Is All You Need"**. It replaced recurrence with **self-attention**, allowing efficient parallelization, longer context handling, and scalable training on unprecedented data volumes. This innovation enabled models like GPT, BERT, and their successors, which demonstrated emergent behaviors at scale, such as few-shot learning and compositional reasoning.

### Core Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   TRANSFORMER ARCHITECTURE                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚        ENCODER          â”‚           DECODER                 â”‚
â”‚                         â”‚                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚   Output Vectors   â”‚  â”‚  â”‚   Output Probs    â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚           â”‚              â”‚           â”‚                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚  Feed-Forward NN  â”‚  â”‚  â”‚  Feed-Forward NN  â”‚           â”‚
â”‚  â”‚  + Layer Norm     â”‚  â”‚  â”‚  + Layer Norm     â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚           â”‚              â”‚           â”‚                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚  Multi-Head       â”‚  â”‚  â”‚  Cross-Attention  â”‚â—„â”€â”€â”€â”€ From â”‚
â”‚  â”‚  Self-Attention   â”‚  â”‚  â”‚  (Encoder-Decoder) â”‚     Encoderâ”‚
â”‚  â”‚  + Layer Norm     â”‚  â”‚  â”‚  + Layer Norm     â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚           â”‚              â”‚           â”‚                       â”‚
â”‚           â”‚              â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚           â”‚              â”‚  â”‚  Masked Multi-Headâ”‚           â”‚
â”‚           â”‚              â”‚  â”‚  Self-Attention   â”‚           â”‚
â”‚           â”‚              â”‚  â”‚  + Layer Norm     â”‚           â”‚
â”‚           â”‚              â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚           â”‚              â”‚           â”‚                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚  Positional       â”‚  â”‚  â”‚  Positional       â”‚           â”‚
â”‚  â”‚  Encoding         â”‚  â”‚  â”‚  Encoding         â”‚           â”‚
â”‚  â”‚  + Input Embed    â”‚  â”‚  â”‚  + Output Embed   â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚           â”‚              â”‚           â”‚                       â”‚
â”‚      [Input Tokens]      â”‚     [Output Tokens]              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Self-Attention Mechanism

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    Input Embeddings    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â–¼             â–¼             â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ W_Q    â”‚   â”‚ W_K    â”‚   â”‚ W_V    â”‚
         â”‚(Query) â”‚   â”‚ (Key)  â”‚   â”‚(Value) â”‚
         â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
             â–¼             â–¼             â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
        â”‚   Q Ã— K^T / âˆšd_k   â”‚          â”‚
        â”‚  (Scaled Dot-Productâ”‚          â”‚
        â”‚     Attention)      â”‚          â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
                   â–¼                     â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
            â”‚   Softmax   â”‚              â”‚
            â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜              â”‚
                   â”‚                     â”‚
                   â–¼                     â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  Attention Weights Ã— V     â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚  Context Vector  â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Variants

#### Encoder-Only: BERT

```
       [CLS] Token1 Token2 ... TokenN [SEP]
           â”‚      â”‚      â”‚          â”‚
           â–¼      â–¼      â–¼          â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚   Token + Segment + Position     â”‚
     â”‚          Embeddings              â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚    Bidirectional Self-Attention  â”‚
     â”‚    Encoder Block Ã— 12 (base)    â”‚  â—„â”€â”€ Sees ALL tokens
     â”‚          or Ã— 24 (large)        â”‚      in BOTH directions
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚   Contextual Representations     â”‚
     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â–¼       â–¼          â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚Classific.â”‚ â”‚ NER â”‚ â”‚   Q&A      â”‚
     â”‚   Head   â”‚ â”‚Head â”‚ â”‚   Head     â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  Pre-training Tasks:
  â€¢ Masked Language Modeling (MLM): Predict [MASK] tokens
  â€¢ Next Sentence Prediction (NSP)
```

#### Decoder-Only: GPT

```
      Token1  Token2  Token3  ...  TokenN
        â”‚       â”‚       â”‚            â”‚
        â–¼       â–¼       â–¼            â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚   Token + Position Embeddings        â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚   Masked (Causal) Self-Attention     â”‚
  â”‚   Decoder Block Ã— 96 (GPT-3)        â”‚  â—„â”€â”€ Each token can
  â”‚   175 Billion Parameters             â”‚      only attend to
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      PREVIOUS tokens
                 â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚   Next Token Prediction              â”‚
  â”‚   P(token_n+1 | token_1...token_n)   â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  Left-to-right autoregressive generation:
  "The cat" â†’ "sat" â†’ "on" â†’ "the" â†’ "mat"
```

### Use Cases

- **NLP:** Text generation, translation, summarization, Q&A
- **Code Generation:** GitHub Copilot, code completion
- **Vision:** Vision Transformers (ViT) for image classification
- **Multimodal:** GPT-4V, Gemini (text + image reasoning)
- **Robotics & RL:** Decision Transformers

---

## 2. Diffusion Models

### Overview

Diffusion models are a class of latent variable generative models consisting of two major components: the **forward diffusion process** (adding noise) and the **reverse sampling process** (removing noise). The model learns to reverse the process of adding noise to generate high-quality outputs in domains such as image generation, text-to-image translation, audio synthesis, and molecular design.

### Core Architecture: Forward and Reverse Process

```
  FORWARD PROCESS (Training â€” Adding Noise)
  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  xâ‚€ â”€â”€â”€â”€â”€â”€â–º xâ‚ â”€â”€â”€â”€â”€â”€â–º xâ‚‚ â”€â”€â”€â”€â”€â”€â–º ... â”€â”€â”€â”€â”€â”€â–º x_T
  (clean)   (+noise)   (+noise)                (pure noise)

  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ğŸ–¼ï¸    â”‚â†’ â”‚ğŸ–¼ï¸+Îµ â”‚â†’ â”‚ğŸ–¼ï¸+ÎµÎµâ”‚â†’  ...  â†’  â”‚ â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ â”‚
  â”‚image â”‚  â”‚      â”‚  â”‚      â”‚            â”‚ Gaussian  â”‚
  â”‚      â”‚  â”‚      â”‚  â”‚      â”‚            â”‚  Noise    â”‚
  â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    t=0       t=1       t=2        ...        t=T


  REVERSE PROCESS (Inference â€” Removing Noise)
  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  x_T â”€â”€â”€â”€â”€â”€â–º x_{T-1} â”€â”€â”€â”€â”€â”€â–º ... â”€â”€â”€â”€â”€â”€â–º xâ‚ â”€â”€â”€â”€â”€â”€â–º xâ‚€
  (noise)    (denoise)                   (denoise)   (clean!)

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”
  â”‚ â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ â”‚â†’ â”‚ğŸ–¼ï¸-Îµ â”‚â†’  ...  â†’  â”‚ğŸ–¼ï¸-Îµ â”‚â†’ â”‚ğŸ–¼ï¸    â”‚
  â”‚ Gaussian  â”‚  â”‚      â”‚            â”‚      â”‚  â”‚resultâ”‚
  â”‚  Noise    â”‚  â”‚      â”‚            â”‚      â”‚  â”‚image â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜
     t=T         t=T-1       ...      t=1       t=0
                    â–²
                    â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  Learned U-Net â”‚
            â”‚  (noise pred.) â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Latent Diffusion Model (Stable Diffusion)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LATENT DIFFUSION MODEL                           â”‚
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚          â”‚    â”‚     LATENT SPACE (compressed)     â”‚  â”‚        â”‚  â”‚
â”‚  â”‚   VAE    â”‚    â”‚                                   â”‚  â”‚  VAE   â”‚  â”‚
â”‚  â”‚ Encoder  â”‚â”€â”€â”€â–ºâ”‚  â”Œâ”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”     â”‚â”€â”€â–ºâ”‚Decoder â”‚  â”‚
â”‚  â”‚          â”‚    â”‚  â”‚z_T â”‚â†’ â”‚z_..â”‚ â†’ â†’  â”‚z_0 â”‚     â”‚  â”‚        â”‚  â”‚
â”‚  â”‚ (Image â†’ â”‚    â”‚  â”‚noiseâ”‚  â”‚    â”‚      â”‚cleanâ”‚    â”‚  â”‚(Latent â”‚  â”‚
â”‚  â”‚  Latent) â”‚    â”‚  â””â”€â”€â”€â”€â”˜  â””â”€â”€â–²â”€â”˜      â””â”€â”€â”€â”€â”˜     â”‚  â”‚â†’ Image)â”‚  â”‚
â”‚  â”‚          â”‚    â”‚             â”‚                      â”‚  â”‚        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                  â”‚     â”‚     U-Net      â”‚            â”‚              â”‚
â”‚                  â”‚     â”‚  (Denoiser)    â”‚            â”‚              â”‚
â”‚                  â”‚     â”‚               â”‚            â”‚              â”‚
â”‚                  â”‚     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚            â”‚              â”‚
â”‚                  â”‚     â”‚  â”‚Cross-Attnâ”‚â—„â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚              â”‚
â”‚                  â”‚     â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚        â”‚   â”‚              â”‚
â”‚                  â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚   â”‚              â”‚
â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”˜              â”‚
â”‚                                                 â”‚                  â”‚
â”‚                                          â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚                                          â”‚ Text Encoder â”‚          â”‚
â”‚                                          â”‚   (CLIP)     â”‚          â”‚
â”‚                                          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                 â”‚                  â”‚
â”‚                                          â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚                                          â”‚ "A cat on a  â”‚          â”‚
â”‚                                          â”‚  beach at    â”‚          â”‚
â”‚                                          â”‚  sunset"     â”‚          â”‚
â”‚                                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### U-Net Architecture (Denoiser)

```
          Input (noisy latent)
                â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚    Conv + Attn      â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚    (64 channels)    â”‚                      â”‚ Skip
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚ Connection
                â”‚ Downsample                      â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
     â”‚    Conv + Attn      â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
     â”‚    (128 channels)   â”‚                 â”‚    â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚    â”‚
                â”‚ Downsample                 â”‚    â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚    â”‚
     â”‚    Conv + Attn      â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚    â”‚
     â”‚    (256 channels)   â”‚            â”‚    â”‚    â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚    â”‚    â”‚
                â”‚ Downsample            â”‚    â”‚    â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚    â”‚    â”‚
     â”‚     BOTTLENECK      â”‚            â”‚    â”‚    â”‚
     â”‚   (512 channels)    â”‚            â”‚    â”‚    â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚    â”‚    â”‚
                â”‚ Upsample              â”‚    â”‚    â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚    â”‚    â”‚
     â”‚  Conv + Attn + Cat  â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚    â”‚
     â”‚    (256 channels)   â”‚                 â”‚    â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚    â”‚
                â”‚ Upsample                   â”‚    â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚    â”‚
     â”‚  Conv + Attn + Cat  â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
     â”‚    (128 channels)   â”‚                      â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
                â”‚ Upsample                        â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
     â”‚  Conv + Attn + Cat  â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚    (64 channels)    â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
       Predicted Noise (Îµ)
```

### Key Variants

| Variant | Description | Example Models |
|---|---|---|
| DDPM | Standard denoising diffusion in pixel space | Original Ho et al. 2020 |
| Latent Diffusion | Diffusion in compressed latent space via VAE | Stable Diffusion, DALL-E 2 |
| Diffusion Transformer (DiT) | Replaces U-Net with Transformer backbone | Stable Diffusion 3, Sora |
| Cascaded Diffusion | Multiple sub-models at increasing resolutions | Imagen |
| Score-based (SGM) | Uses score matching + Langevin dynamics | Song & Ermon 2019 |

### Use Cases

- **Image Generation:** Stable Diffusion, DALL-E 2/3, Midjourney
- **Video Generation:** Sora, Runway Gen-3, Make-A-Video
- **Image Editing:** Inpainting, outpainting, super-resolution
- **Audio Synthesis:** Music and speech generation
- **Molecular Design:** Drug discovery and protein structure
- **3D Generation:** Stable Video 4D, 3D object synthesis

---

## 3. Generative Adversarial Networks (GANs)

### Overview

Introduced by Ian Goodfellow in **2014**, GANs consist of two neural networks â€” a **generator** and a **discriminator** â€” trained simultaneously in an adversarial competition. The generator produces new samples while the discriminator aims to distinguish between fake and real data. By incorporating this adversarial competition, GANs can produce remarkably realistic and diverse image and video samples.

### Core Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     GAN ARCHITECTURE                         â”‚
â”‚                                                              â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Random   â”‚      â”‚             â”‚      â”‚              â”‚   â”‚
â”‚  â”‚  Noise    â”‚â”€â”€â”€â”€â”€â–ºâ”‚  GENERATOR  â”‚â”€â”€â”€â”€â”€â–ºâ”‚  Generated   â”‚   â”‚
â”‚  â”‚  Vector   â”‚      â”‚     (G)     â”‚      â”‚  Image       â”‚   â”‚
â”‚  â”‚  (z~N)    â”‚      â”‚             â”‚      â”‚  G(z)        â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                  â”‚           â”‚
â”‚                                                  â–¼           â”‚
â”‚                                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚                                          â”‚              â”‚   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚DISCRIMINATOR â”‚   â”‚
â”‚  â”‚  Real        â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚     (D)      â”‚   â”‚
â”‚  â”‚  Images      â”‚                        â”‚              â”‚   â”‚
â”‚  â”‚  (training)  â”‚                        â”‚  Real or     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚  Fake?       â”‚   â”‚
â”‚                                          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                  â”‚           â”‚
â”‚                                                  â–¼           â”‚
â”‚                                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚                                          â”‚  D(x) â†’ 1   â”‚   â”‚
â”‚                                          â”‚  (Real)      â”‚   â”‚
â”‚                                          â”‚  D(G(z)) â†’ 0 â”‚   â”‚
â”‚                                          â”‚  (Fake)      â”‚   â”‚
â”‚                                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                              â”‚
â”‚  â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Feedback Loop (Backpropagation) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º  â”‚
â”‚  G tries to MAXIMIZE D's error | D tries to MINIMIZE error  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### GAN Training Flow

```
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚                GAN TRAINING LOOP                      â”‚
  â”‚                                                       â”‚
  â”‚  Step 1: Train Discriminator                          â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”   â”Œâ”€â”€â”€â”                          â”‚
  â”‚  â”‚Real(x)â”‚â”€â”€â–ºâ”‚ D â”‚â”€â”€â–ºâ”‚ 1 â”‚  (label: real)           â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”˜   â””â”€â”€â”€â”˜                          â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”   â”Œâ”€â”€â”€â”   â”Œâ”€â”€â”€â”                  â”‚
  â”‚  â”‚Noise zâ”‚â”€â”€â–ºâ”‚ G â”‚â”€â”€â–ºâ”‚ D â”‚â”€â”€â–ºâ”‚ 0 â”‚  (label: fake)   â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”˜   â””â”€â”€â”€â”˜   â””â”€â”€â”€â”˜                  â”‚
  â”‚  Update D weights only âœ“                              â”‚
  â”‚                                                       â”‚
  â”‚  Step 2: Train Generator                              â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”   â”Œâ”€â”€â”€â”                          â”‚
  â”‚  â”‚Noise zâ”‚â”€â”€â–ºâ”‚ G â”‚â”€â”€â–ºâ”‚ D â”‚â”€â”€â–º wants D to say "1"    â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”˜   â””â”€â”€â”€â”˜   (fool the discriminator)â”‚
  â”‚  Update G weights only âœ“                              â”‚
  â”‚                                                       â”‚
  â”‚  Repeat until equilibrium (Nash Equilibrium)          â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Notable GAN Variants

```
  GAN Evolution Timeline:
  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  2014          2015          2016          2017       2018+
   â”‚             â”‚             â”‚             â”‚          â”‚
   â–¼             â–¼             â–¼             â–¼          â–¼
  GAN â”€â”€â”€â–º DCGAN â”€â”€â”€â–º Pix2Pix â”€â”€â”€â–º WGAN â”€â”€â”€â–º StyleGAN
  (Original)  (Deep       (Image-to-  (Wasserstein (High-res
              Convol.)    Image)       distance)   face gen.)

                                              â”€â”€â”€â–º CycleGAN
                                              (Unpaired image
                                               translation)

                                              â”€â”€â”€â–º ProGAN
                                              (Progressive
                                               growing)

                                              â”€â”€â”€â–º BigGAN
                                              (Large-scale
                                               class-conditional)
```

### Strengths & Challenges

| Strengths | Challenges |
|---|---|
| Sharp, high-fidelity outputs | Training instability |
| Excellent for realistic images | Mode collapse |
| Fast inference (single pass) | Hard to find equilibrium |
| Good for style transfer | No explicit density estimation |

### Use Cases

- **Image Synthesis:** Photorealistic face generation (StyleGAN)
- **Image-to-Image Translation:** Style transfer, colorization
- **Super Resolution:** Enhancing low-resolution images
- **Data Augmentation:** Generating synthetic training data
- **Video Generation:** Face swap, deepfake technology
- **Medical Imaging:** Synthetic scans for training

---

## 4. Variational Autoencoders (VAEs)

### Overview

Variational Autoencoders were introduced in **2014** by Diederik P. Kingma and Max Welling. VAEs are generative models explicitly designed to capture the underlying probability distribution of a given dataset and generate novel samples. They consist of two major components: an **encoder** that maps data to a latent distribution, and a **decoder** that reconstructs data from sampled latent vectors.

### Core Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    VAE ARCHITECTURE                               â”‚
â”‚                                                                   â”‚
â”‚       INPUT                 LATENT SPACE              OUTPUT      â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚          â”‚    â”‚                      â”‚    â”‚              â”‚   â”‚
â”‚  â”‚  Input   â”‚    â”‚    Î¼ (mean)          â”‚    â”‚  Reconstructedâ”‚  â”‚
â”‚  â”‚  Image   â”‚â”€â”€â–ºâ”‚â”€â”€â–ºâ”€â”€â”€â”€â”€â”€â”            â”‚â”€â”€â–ºâ”‚  Image        â”‚   â”‚
â”‚  â”‚  (x)     â”‚    â”‚        â–¼            â”‚    â”‚  (xÌ‚)         â”‚   â”‚
â”‚  â”‚          â”‚    â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚    â”‚              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚    â”‚ Sample â”‚       â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚       â”‚          â”‚    â”‚z=Î¼+ÏƒÂ·Îµ â”‚       â”‚          â–²            â”‚
â”‚       â–¼          â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚          â”‚            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚        â–²            â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚          â”‚    â”‚    Ïƒ (std dev)       â”‚    â”‚          â”‚      â”‚
â”‚  â”‚ ENCODER  â”‚    â”‚                      â”‚    â”‚ DECODER  â”‚      â”‚
â”‚  â”‚ q(z|x)   â”‚    â”‚   Îµ ~ N(0, I)       â”‚    â”‚ p(x|z)   â”‚      â”‚
â”‚  â”‚          â”‚    â”‚   (random noise)     â”‚    â”‚          â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                                   â”‚
â”‚  Loss = Reconstruction Loss + KL Divergence                      â”‚
â”‚       = ||x - xÌ‚||Â² + KL(q(z|x) || p(z))                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### VAE Data Flow

```
                      ENCODING
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚   Input   â”‚    â”‚  Neural   â”‚    â”‚ Distribution     â”‚
  â”‚   Data    â”‚â”€â”€â”€â–ºâ”‚  Network  â”‚â”€â”€â”€â–ºâ”‚ Parameters       â”‚
  â”‚   (x)     â”‚    â”‚ (Encoder) â”‚    â”‚ Î¼ = [0.2, -1.5] â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ Ïƒ = [0.3,  0.8] â”‚
                                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚
                                    Reparameterization
                                     z = Î¼ + Ïƒ Â· Îµ
                                              â”‚
                                              â–¼
                      DECODING       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚  Latent      â”‚
  â”‚ Generated â”‚â—„â”€â”€â”€â”‚  Neural   â”‚â—„â”€â”€â”€â”‚  Vector (z)  â”‚
  â”‚   Data    â”‚    â”‚  Network  â”‚    â”‚  [0.5, -0.3] â”‚
  â”‚   (xÌ‚)    â”‚    â”‚ (Decoder) â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  Latent Space Visualization:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚           â€¢  â€¢   â€¢              â”‚
  â”‚        â€¢   CATS  â€¢  â€¢           â”‚
  â”‚      â€¢  â€¢  â€¢  â€¢   â€¢            â”‚
  â”‚                                 â”‚  â—„â”€â”€ Smooth, continuous
  â”‚          â€¢  â€¢                   â”‚      latent space allows
  â”‚       â€¢  DOGS  â€¢  â€¢            â”‚      interpolation between
  â”‚     â€¢  â€¢  â€¢  â€¢   â€¢             â”‚      concepts
  â”‚                                 â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Strengths & Challenges

| Strengths | Challenges |
|---|---|
| Stable training | Blurrier outputs than GANs |
| Smooth, interpretable latent space | Limited sharpness |
| Good for interpolation | Trade-off: reconstruction vs. regularization |
| Probabilistic framework | Can miss fine details |
| Diverse outputs | |

### Use Cases

- **Image Generation:** Novel image synthesis
- **Anomaly Detection:** Fraud detection, medical diagnosis (tumors, diseases)
- **Data Compression:** Dimensionality reduction
- **Drug Discovery:** Molecular generation
- **Signal Analysis:** Stock market patterns, health monitoring
- **Content Generation:** 3D models from 2D images, handwritten text
- **Key Role in Diffusion Models:** VAE encoder/decoder used in Stable Diffusion's latent space

---

## 5. Multimodal Vision-Language Models

### Overview

Vision Language Models (VLMs) bridge the gap between visual and linguistic understanding of AI. They consist of a multimodal architecture that learns to associate information from image and text modalities. By using advances in Transformers and pretraining strategies, VLMs unlock the potential of vision-language applications ranging from image search to image captioning, generative tasks, and more.

### General VLM Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              VISION-LANGUAGE MODEL (General)                  â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚           â”‚                     â”‚           â”‚            â”‚
â”‚  â”‚   Image   â”‚                     â”‚   Text    â”‚            â”‚
â”‚  â”‚   Input   â”‚                     â”‚   Input   â”‚            â”‚
â”‚  â”‚           â”‚                     â”‚           â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜            â”‚
â”‚        â”‚                                 â”‚                   â”‚
â”‚        â–¼                                 â–¼                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚  Vision   â”‚                     â”‚   Text    â”‚            â”‚
â”‚  â”‚  Encoder  â”‚                     â”‚  Encoder  â”‚            â”‚
â”‚  â”‚  (ViT /   â”‚                     â”‚  (BERT /  â”‚            â”‚
â”‚  â”‚  ResNet)  â”‚                     â”‚  GPT)     â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜            â”‚
â”‚        â”‚                                 â”‚                   â”‚
â”‚        â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚                   â”‚
â”‚        â””â”€â”€â”€â–ºâ”‚   Multimodal      â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚             â”‚   Fusion Module   â”‚                            â”‚
â”‚             â”‚  (Cross-Attention/â”‚                            â”‚
â”‚             â”‚   Dot Product/    â”‚                            â”‚
â”‚             â”‚   Co-Attention)   â”‚                            â”‚
â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â”‚                      â”‚                                       â”‚
â”‚                      â–¼                                       â”‚
â”‚             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚             â”‚  Task-Specific     â”‚                           â”‚
â”‚             â”‚  Output Head /     â”‚                           â”‚
â”‚             â”‚  LLM Decoder       â”‚                           â”‚
â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â”‚                      â”‚                                       â”‚
â”‚                      â–¼                                       â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚         â”‚ Caption / Answer / Label  â”‚                        â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### CLIP Architecture (Contrastive Learning)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            CLIP: Contrastive Language-Image Pre-training      â”‚
â”‚                                                              â”‚
â”‚   IMAGE BATCH                        TEXT BATCH              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ Dog ğŸ• â”‚                        â”‚ "A photo of  â”‚         â”‚
â”‚  â”‚ Cat ğŸˆ â”‚                        â”‚  a dog"      â”‚         â”‚
â”‚  â”‚ Car ğŸš— â”‚                        â”‚ "A cute cat" â”‚         â”‚
â”‚  â”‚  ...   â”‚                        â”‚ "Red car"    â”‚         â”‚
â”‚  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚      â”‚                                     â”‚                 â”‚
â”‚      â–¼                                     â–¼                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚ Image  â”‚                        â”‚   Text    â”‚            â”‚
â”‚  â”‚Encoder â”‚                        â”‚  Encoder  â”‚            â”‚
â”‚  â”‚ (ViT)  â”‚                        â”‚(Transf.)  â”‚            â”‚
â”‚  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                        â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜            â”‚
â”‚      â”‚                                    â”‚                  â”‚
â”‚      â–¼                                    â–¼                  â”‚
â”‚  [Iâ‚, Iâ‚‚, Iâ‚ƒ]                    [Tâ‚, Tâ‚‚, Tâ‚ƒ]             â”‚
â”‚                                                              â”‚
â”‚        Cosine Similarity Matrix:                             â”‚
â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
â”‚        â”‚       Tâ‚    Tâ‚‚    Tâ‚ƒ   â”‚                          â”‚
â”‚        â”‚  Iâ‚ [ âœ…    âŒ    âŒ ] â”‚  Diagonal = MAXIMIZE     â”‚
â”‚        â”‚  Iâ‚‚ [ âŒ    âœ…    âŒ ] â”‚  Off-diag  = MINIMIZE    â”‚
â”‚        â”‚  Iâ‚ƒ [ âŒ    âŒ    âœ… ] â”‚                          â”‚
â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â”‚                                                              â”‚
â”‚  â†’ Contrastive Loss pushes matching pairs closer             â”‚
â”‚    and non-matching pairs apart in embedding space            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### LLaVA Architecture (Visual Instruction Tuning)

```
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚                    LLaVA Architecture                      â”‚
  â”‚                                                            â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
  â”‚  â”‚          â”‚       â”‚  Linear  â”‚       â”‚              â”‚  â”‚
  â”‚  â”‚  Image   â”‚â”€â”€â”€â”€â”€â”€â–ºâ”‚Projectionâ”‚â”€â”€â”€â”€â”€â”€â–ºâ”‚              â”‚  â”‚
  â”‚  â”‚          â”‚       â”‚  Layer   â”‚       â”‚              â”‚  â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚    Large     â”‚  â”‚
  â”‚       â”‚                                â”‚   Language   â”‚  â”‚
  â”‚  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”                          â”‚    Model     â”‚â”€â”€â–º  Response
  â”‚  â”‚  CLIP    â”‚   Visual Tokens          â”‚  (Vicuna /   â”‚
  â”‚  â”‚  ViT     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚   LLaMA)    â”‚
  â”‚  â”‚ Encoder  â”‚                          â”‚              â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚              â”‚
  â”‚                                         â”‚              â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚              â”‚
  â”‚  â”‚ "Describe this image"â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚              â”‚
  â”‚  â”‚  (Text Instruction)  â”‚  Text Tokens â”‚              â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚                                                            â”‚
  â”‚  Training:                                                 â”‚
  â”‚  Stage 1: Pre-train projection layer (image-text pairs)    â”‚
  â”‚  Stage 2: Fine-tune end-to-end (instruction following)     â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Use Cases

- **Zero-Shot Image Classification:** CLIP
- **Visual Question Answering (VQA):** LLaVA, GPT-4V
- **Image Captioning:** BLIP, Flamingo
- **Image Search & Retrieval:** CLIP embeddings
- **Multimodal Chat:** GPT-4V, Gemini

---

## 6. Hybrid and Emerging Architectures

### Overview

Hybrid methodologies integrate multiple distinct generative frameworks to synergistically combine the advantageous characteristics of each constituent model while mitigating their inherent limitations.

### VAE-GAN Hybrid

```
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚                    VAE-GAN HYBRID                        â”‚
  â”‚                                                          â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
  â”‚  â”‚Input â”‚â”€â”€â–ºâ”‚Encoderâ”‚â”€â”€â–ºâ”‚Latent zâ”‚â”€â”€â–ºâ”‚  Decoder/    â”‚ â”‚
  â”‚  â”‚Image â”‚   â”‚ (VAE) â”‚   â”‚        â”‚   â”‚  Generator   â”‚ â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
  â”‚                                              â”‚          â”‚
  â”‚                                              â–¼          â”‚
  â”‚             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
  â”‚             â”‚  Real Image  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚              â”‚  â”‚
  â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚Discriminator â”‚  â”‚
  â”‚                                      â”‚   (GAN)      â”‚  â”‚
  â”‚             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚              â”‚  â”‚
  â”‚             â”‚ Generated    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  Real/Fake?  â”‚  â”‚
  â”‚             â”‚ Image        â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
  â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
  â”‚                                                          â”‚
  â”‚  Benefits: VAE's structured latent space                 â”‚
  â”‚          + GAN's sharp, realistic outputs                â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Diffusion Transformer (DiT)

```
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚         DIFFUSION TRANSFORMER (DiT)                  â”‚
  â”‚                                                      â”‚
  â”‚  Replaces U-Net with Transformer in Diffusion Model  â”‚
  â”‚                                                      â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚
  â”‚  â”‚ Noisy Latent â”‚                                   â”‚
  â”‚  â”‚    Patches   â”‚                                   â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
  â”‚         â”‚                                            â”‚
  â”‚         â–¼                                            â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚
  â”‚  â”‚ Patchify +   â”‚  (divide into patches/tokens)     â”‚
  â”‚  â”‚ Embed        â”‚                                   â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
  â”‚         â”‚                                            â”‚
  â”‚         â–¼                                            â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
  â”‚  â”‚ Transformer  â”‚â—„â”€â”€â”‚Timestep tâ”‚                    â”‚
  â”‚  â”‚ Blocks Ã— N   â”‚â—„â”€â”€â”‚+ Class   â”‚                    â”‚
  â”‚  â”‚(Self-Attn +  â”‚   â”‚Condition â”‚                    â”‚
  â”‚  â”‚ Cross-Attn + â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
  â”‚  â”‚ Feed-Forward)â”‚                                   â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
  â”‚         â”‚                                            â”‚
  â”‚         â–¼                                            â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚
  â”‚  â”‚ Predicted    â”‚                                   â”‚
  â”‚  â”‚ Noise / v    â”‚                                   â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
  â”‚                                                      â”‚
  â”‚  Used in: Stable Diffusion 3, Sora, Movie Gen       â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### State-Space Models (Mamba) â€” Emerging Alternative

```
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚         STATE-SPACE MODELS (SSMs / Mamba)            â”‚
  â”‚                                                      â”‚
  â”‚  Alternative to Transformers for long sequences      â”‚
  â”‚                                                      â”‚
  â”‚  Input â”€â”€â–º [Selective State Space] â”€â”€â–º Output        â”‚
  â”‚                                                      â”‚
  â”‚  â€¢ Linear scaling with sequence length (vs O(nÂ²))   â”‚
  â”‚  â€¢ Efficient for very long contexts                  â”‚
  â”‚  â€¢ Input-dependent selection mechanism               â”‚
  â”‚  â€¢ Emerging use in language + vision tasks           â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 7. Architecture Comparison

### At-a-Glance Comparison

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Architecture   â”‚  Primary     â”‚  Training    â”‚  Output       â”‚  Main Use    â”‚
â”‚                 â”‚  Mechanism   â”‚  Stability   â”‚  Quality      â”‚  Cases       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Transformer    â”‚  Self-       â”‚  High        â”‚  SOTA for     â”‚  NLP, code,  â”‚
â”‚  (GPT/BERT)     â”‚  Attention   â”‚              â”‚  text         â”‚  multimodal  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Diffusion      â”‚  Iterative   â”‚  Very High   â”‚  SOTA for     â”‚  Image/video â”‚
â”‚  Models         â”‚  Denoising   â”‚              â”‚  images       â”‚  generation  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  GANs           â”‚  Adversarial â”‚  Variable    â”‚  Sharp,       â”‚  Image syn., â”‚
â”‚                 â”‚  Game        â”‚  (unstable)  â”‚  realistic    â”‚  style xfer  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  VAEs           â”‚  Variational â”‚  High        â”‚  Smooth but   â”‚  Anomaly det,â”‚
â”‚                 â”‚  Inference   â”‚              â”‚  blurrier     â”‚  compression â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  VLMs (CLIP,    â”‚  Contrastive â”‚  High        â”‚  Multimodal   â”‚  Image+Text  â”‚
â”‚   LLaVA)        â”‚  + Attention â”‚              â”‚  understandingâ”‚  reasoning   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Hybrids        â”‚  Combined    â”‚  Varies      â”‚  Best of      â”‚  Specialized â”‚
â”‚  (VAE-GAN, DiT) â”‚  methods     â”‚              â”‚  both worlds  â”‚  tasks       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Evolution Timeline

```
  2014          2017          2018      2020       2022         2024+
   â”‚             â”‚             â”‚         â”‚          â”‚            â”‚
   â”œâ”€ GAN        â”œâ”€ Transformerâ”œâ”€ BERT   â”œâ”€ DDPM    â”œâ”€ Stable    â”œâ”€ Sora
   â”œâ”€ VAE        â”‚             â”œâ”€ GPT-1  â”œâ”€ GPT-3   â”‚  Diffusion â”œâ”€ SD 3
   â”‚             â”‚             â”‚         â”œâ”€ CLIP    â”œâ”€ DALL-E 2  â”œâ”€ GPT-4o
   â”‚             â”‚             â”‚         â”‚          â”œâ”€ ChatGPT   â”œâ”€ DiT
   â”‚             â”‚             â”‚         â”‚          â”œâ”€ Midjourneyâ”œâ”€ Gemini
   â”‚             â”‚             â”‚         â”‚          â”‚            â”œâ”€ LLaMA 3
   â–¼             â–¼             â–¼         â–¼          â–¼            â–¼
  [Foundation]  [Attention     [Pre-     [Scale     [Consumer    [Multi-
                 Revolution]   training]  Era]       AI Boom]     modal Era]
```

---

## References

1. Vaswani et al., "Attention Is All You Need" (2017) â€” Transformer architecture
2. Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers" (2018)
3. Radford et al., GPT series (2018-2023) â€” OpenAI
4. Ho et al., "Denoising Diffusion Probabilistic Models" (2020)
5. Rombach et al., "High-Resolution Image Synthesis with Latent Diffusion Models" (2021)
6. Goodfellow et al., "Generative Adversarial Networks" (2014)
7. Kingma & Welling, "Auto-Encoding Variational Bayes" (2014)
8. Radford et al., "Learning Transferable Visual Models From Natural Language Supervision" (2021) â€” CLIP
9. Liu et al., "Visual Instruction Tuning" (2023) â€” LLaVA
10. Peebles & Xie, "Scalable Diffusion Models with Transformers" (2023) â€” DiT

---

*Last updated: February 2026*
